---
title: "Large language models (LLMs)"
subtitle: "The quick intro"
author: "sma@hvl.no"
date: "October 23, 2025"
format:
  revealjs:
    theme: [default, custom.scss]
    auto-stretch: false
    auto-play-media: true
    slide-number: true
    code-line-numbers: false
    progress: false
    preload-iframes: true
highlight-style: atom-one
revealjs-plugins:
  - pointer
format-links: [html]
---



## {data-menu-title="ChatGPT" .center background-color="#E0E0E0"}
<!-- The prime LLM example --> 

:::{style="background-color: #fff; box-shadow: 10px 10px 10px rgba(0, 0, 0, 0.15); border: 1px solid #9E9E9E;"}
![](figures/chatgpt.png){fig-align="center"}
:::


## {data-menu-title="AI trends" .center}
<!-- Most of the recent models are LLMs -->

![](figures/epoch-ml-trends.svg)



## {data-menu-title="Stanford AI index" .center }
<!-- AI performance as of 2025 -->

![](figures/fig_2.1.33.png){fig-align="center" width="800px"}

:::{.absolute bottom="0%" left="5%" style="font-size: 0.5em;"}
[Stanford AI index](https://hai.stanford.edu/ai-index/2025-ai-index-report)
:::


## {data-menu-title="GPT-2" background-iframe="https://openai.com/index/gpt-2-1-5b-release/" background-interactive="true"}
<!-- Let's rewind time! -->


## {data-menu-title="Demo GPT-2" background-color="#E0E0E0"}
<!-- Terminal -->



## {data-menu-title="Attention is all you need" background-image="figures/attention-arxiv.png" background-size="contain"}
<!-- Introduce the innovative tech -->

:::{.absolute top="0%" left="40%" width="450px"}
![](figures/attention.png){style="background-color: #fff; box-shadow: 10px 10px 10px rgba(0, 0, 0, 0.15); border: 1px solid #9E9E9E;"}
:::


## _Step 1:_ Text to numbers


::::{.columns}
:::{.column width="50%"}

:::{style="padding-top: 60px;"}
:::

Computers know only [numbers]{.purple}

Text contains... [text]{.purple}

Need to make text into numbers (_aka_ **tokens**).

:::{style="padding-top: 60px;"}
:::

:::{.fragment fragment-index=1}
Different language models often use different tokenisation algorithms
:::


:::
:::{.column width="50%"}
:::{.fragment fragment-index=1}
![Tokeniser [playground](https://huggingface.co/spaces/Xenova/the-tokenizer-playground)](figures/tokenizers.png)
:::
:::
::::


## {data-menu-title="Vocabulary"}

{{< include vocab.qmd >}}


## _Step 2:_ Text to <s>numbers</s> _decimal_ numbers

:::{style="padding-top: 20px;"}
:::
Let a one-layer neural network convert the indices to a [vector]{.purple}:

::::{.columns}
:::{.column width="50%"}
:::{.absolute top="44%" left="2.5%" width="380px"}
![](figures/embedding-layer.png)
:::

:::{.absolute top="24%" left="0%" style="text-align: center;"}
`the`<br>
<i class="bi bi-arrow-down"></i><br>
`[ 0     1     0     0 ]`
:::

:::{.absolute bottom="4%" left="2%"}
`[ -1.46 -0.86  0.09 ]`
:::
:::

:::{.column width="50%"}
:::{.fragment style="padding-top:160px;"}
```{.python}
my_embedding = {
  'the': [-1.46, -0.86,  0.09],
  'and': [-0.27,  1.15,  1.19],
  'a':   [ 1.17,  0.06, -0.16],
  'of':  [ 0.60,  0.10,  0.22],
  ...
}
```
:::
:::
::::


## _Step 3:_ Give the numbers some meaning


:::{style="padding-top: 70px;"}
:::

::::{.columns}
:::{.column width="50%"}
Our words are now _embedded_ in an $\small N$-dimensional vector space

- Google Gemini: $\small N = 4096$
- GPT-2 (our example): $\small N = 768$

This means [_vector arithmetic_]{.pink} now applies
:::

:::{.column width="45%" style="text-align: right;"}
![](figures/vectorspace-1.png)
:::
::::

:::{.fragment}
Training our language model yields a [spatial relationship]{.blue} between related words
:::


## {data-menu-title="Demo" background-color="#FFF8E1"}
<!-- TensorBoard demo -->


## {data-menu-title="word2vec"}

:::{.r-stack}

:::{.fragment fragment-index=0}
![](figures/word2vec/word2vec-1.png){width="70%"}
:::

:::{.fragment fragment-index=1}
![](figures/word2vec/word2vec-2.png){width="70%"}
:::

:::{.fragment fragment-index=2}
![](figures/word2vec/word2vec-3.png){width="70%"}
:::

:::{.fragment fragment-index=3}
![](figures/word2vec/word2vec-4.png){width="70%"}
:::

:::

:::{.fragment fragment-index=3 .absolute top="0px" right="0px" width="350px"}
![](figures/roma.webp)
:::



## {data-menu-title="Other uses of embeddings"}
<!-- Vector embeddings are not new, but used other places too -->

![](figures/google-vector-search.png){width="650px" fig-align="center"}


:::{layout-ncol=2}
![](https://storage.googleapis.com/gweb-cloudblog-publish/images/Matching_Engine_blog_visuals_1.max-2000x2000.png)

![](https://storage.googleapis.com/gweb-cloudblog-publish/images/Matching_Engine_blog_visuals_3.max-2000x2000.png)
:::


:::{.absolute bottom="0%" right="0%" style="font-size: 0.5em;"}
[cloud.google.com/blog](https://cloud.google.com/blog/topics/developers-practitioners/find-anything-blazingly-fast-googles-vector-search-technology)
:::




## {data-menu-title="(Self-)Attention is all you need" background-image="figures/attention-arxiv.png" background-size="contain"}

:::{.absolute top="0%" left="40%" width="450px"}
![](figures/attention.png){style="background-color: #fff; box-shadow: 10px 10px 10px rgba(0, 0, 0, 0.15); border: 1px solid #9E9E9E;"}
:::



## Self-attention

:::{style="padding-top: 30px;"}
:::
What service does a "station" provide?

:::{style="padding-top: 30px;"}
:::

- [He stopped at the **station** to fill petrol.]{.purple}
- [The meteorological **station** is unmanned.]{.deep-purple}
- [The train left the **station** on time.]{.blue}
- [I prefer BBC Radio 4 over other **station**s.]{.teal}

Of course dependent on context



## Self-attention

Example sentence: _"The train left the station on time"_

![](figures/attention-chollet.png){fig-align="center" width="900px"}

:::{.absolute top="400%" right="0%" style="font-size: 0.7em;"}
<i class="bi bi-arrow-clockwise"></i>Repeat<br>for all tokens
:::


## Self-attention

::::{.columns}
:::{.column width="74%"}
In (slow) code, we can write it as

:::{style="padding-top: 10px;"}
:::

```{.python code-line-numbers="true"}
def self_attention(input_sequence):
  output = tf.zeros(shape=input_sequence.shape)

  for i, vector in enumerate(input_sequence):
    scores = tf.zeros(shape=(len(input_sequence), ))

    for j, other_vector in enumerate(input_sequence):
      scores[j] = tf.tensordot(vector, other_vector, axis=1)

    scores /= np.sqrt(input_sequence.shape[1])
    scores = tf.nn.softmax(scores)

    new_representation = tf.zeros(shape=vector.shape)
    for j, other_vector in enumerate(input_sequence):
      new_representation += other_vector * scores[j]

    output[i] = new_representation

  return output
```
:::

:::{.column width="26%"}
:::
::::

## Self-attention

::::{.columns}
:::{.column width="72%"}
In (slow) code, we can write it as

:::{style="padding-top: 10px;"}
:::

```{.python code-line-numbers="4-8"}
def self_attention(input_sequence):
  output = tf.zeros(shape=input_sequence.shape)

  for i, vector in enumerate(input_sequence):
    scores = tf.zeros(shape=(len(input_sequence), ))

    for j, other_vector in enumerate(input_sequence):
      scores[j] = tf.tensordot(vector, other_vector, axis=1)

    scores /= np.sqrt(input_sequence.shape[1])
    scores = tf.nn.softmax(scores)

    new_representation = tf.zeros(shape=vector.shape)
    for j, other_vector in enumerate(input_sequence):
      new_representation += other_vector * scores[j]

    output[i] = new_representation

  return output
```
:::

:::{.column width="28%"}
:::{style="padding-top: 120px;"}
:::

Compute dot product with all other token vectors

:::{.r-stack}

![](figures/dotprod-1.png){.fragment .fade-in-then-out}

![](figures/dotprod-2.png){.fragment .fade-in-then-out}

![](figures/dotprod-3.png){.fragment .fade}

:::
:::
::::

## Self-attention

::::{.columns}
:::{.column width="74%"}
In (slow) code, we can write it as

:::{style="padding-top: 10px;"}
:::

```{.python code-line-numbers="10-11"}
def self_attention(input_sequence):
  output = tf.zeros(shape=input_sequence.shape)

  for i, vector in enumerate(input_sequence):
    scores = tf.zeros(shape=(len(input_sequence), ))

    for j, other_vector in enumerate(input_sequence):
      scores[j] = tf.tensordot(vector, other_vector, axis=1)

    scores /= np.sqrt(input_sequence.shape[1])
    scores = tf.nn.softmax(scores)

    new_representation = tf.zeros(shape=vector.shape)
    for j, other_vector in enumerate(input_sequence):
      new_representation += other_vector * scores[j]

    output[i] = new_representation

  return output
```
:::

:::{.column width="26%"}
:::{style="padding-top: 200px;"}
Scale by input length and apply softmax

Softmax ensures all attention scores are in the interval \[0, 1\].
:::
:::
::::

## Self-attention

::::{.columns}
:::{.column width="74%"}
In (slow) code, we can write it as

:::{style="padding-top: 10px;"}
:::

```{.python code-line-numbers="13-15"}
def self_attention(input_sequence):
  output = tf.zeros(shape=input_sequence.shape)

  for i, vector in enumerate(input_sequence):
    scores = tf.zeros(shape=(len(input_sequence), ))

    for j, other_vector in enumerate(input_sequence):
      scores[j] = tf.tensordot(vector, other_vector, axis=1)

    scores /= np.sqrt(input_sequence.shape[1])
    scores = tf.nn.softmax(scores)

    new_representation = tf.zeros(shape=vector.shape)
    for j, other_vector in enumerate(input_sequence):
      new_representation += other_vector * scores[j]

    output[i] = new_representation

  return output
```
:::

:::{.column width="26%"}
:::{style="padding-top: 320px;"}
Multiply each token vector by the score, and sum all of them
:::
:::
::::

## Self-attention

::::{.columns}
:::{.column width="74%"}
In (slow) code, we can write it as

:::{style="padding-top: 10px;"}
:::

```{.python code-line-numbers="true"}
def self_attention(input_sequence):
  output = tf.zeros(shape=input_sequence.shape)
 
  for i, vector in enumerate(input_sequence):
    scores = tf.zeros(shape=(len(input_sequence), ))

    for j, other_vector in enumerate(input_sequence):
      scores[j] = tf.tensordot(vector, other_vector, axis=1)

    scores /= np.sqrt(input_sequence.shape[1])
    scores = tf.nn.softmax(scores)

    new_representation = tf.zeros(shape=vector.shape)
    for j, other_vector in enumerate(input_sequence):
      new_representation += other_vector * scores[j]

    output[i] = new_representation

  return output
```
:::

:::{.column width="26%"}
:::{style="padding-top: 400px;"}
Return the new vector representation
:::
:::
::::


## The _query-key-value_ model

:::{style="padding-top: 10px;"}
:::

We can generalise the attention mechanism by using concepts from information retrieval.

What we computed was basically

::::{.columns}
:::{.column width=15%}
:::
:::{.column width=70%}
:::{style="padding-top: 20px;"}
:::
```{.python}
output = sum(inputs * pairwise_scores(inputs, inputs))
```
:::
::::

:::{.fragment}
but we could be doing this with three different sequences:

::::{.columns}
:::{.column width=15%}
:::
:::{.column width=70%}
:::{style="padding-top: 20px;"}
:::
```{.python}
output = sum(values * pairwise_scores(query, keys))
```
:::
::::

> For each element in a query, compute how much it is related to every key, and use these scores to weight a sum of values

:::



## <s>Self</s> attention

::::{.columns}
:::{.column width="70%"}
Now we want to increase the complexity by computing attention several times in parallel.

[_But:_]{.color-red-dark} Our transformation so far has no learnable parameters

 <i class="bi bi-arrow-right"></i> it's just a stateless operation, we get the same result each time.

:::{.fragment}
Key idea from the [_"Attention is all you need"_]{.color-deep-purple} paper:

- Pass each input (query, key and value) though a separate neural network layer
  - Each with their own parameters

<i class="bi bi-arrow-right"></i> This is where the model **learns the different meanings** of the words in a sentence.
:::
::::
:::

:::{.absolute top="-5%" left="82%" width="1800px"}
![](figures/attention-arxiv.png)
:::


## {data-menu-title="Attention visualisation"}

:::{style="padding-left: 20%;"}
<iframe width="800px" height="800px" src="figures/attention_plot.html"></iframe>
:::

## Vector embeddings

Tokenise and embed the words:

:::{.absolute top="50%" left="10%"}
"you are **right**"
:::

:::{.absolute top="30%" left="45%" width="500px"}
![](figures/vectorspace-1.png)
:::


## _Transformers_

:::{.absolute top="30%" left="0%"}
"you are **right**"
:::

:::{.absolute top="80%" left="0%"}
"turn **right** here"
:::

:::{.absolute top="15%" left="25%" width="360px"}
![](figures/vectorspace-1.png)
:::

:::{.absolute top="15%" right="-5%" width="360px" .fragment fragment-index=1}
![](figures/vectorspace-2.png)
:::

:::{.absolute bottom="0%" left="25%" width="360px"}
![](figures/vectorspace-3.png)
:::

:::{.absolute bottom="0%" right="-5%" width="360px" .fragment fragment-index=1}
![](figures/vectorspace-4.png)
:::

:::{.absolute top="30%" left="60%" style="text-align: center; font-size:0.8em;" .fragment fragment-index=1}
transform!<br>
<i class="bi bi-arrow-right"></i>
:::

:::{.absolute top="70%" left="60%" style="text-align: center; font-size: 0.8em;" .fragment fragment-index=1}
transform!<br>
<i class="bi bi-arrow-right"></i>
:::


:::{.absolute bottom="-5%" left="35%" style="font-size: 0.6em;"}
Embedding space
:::

:::{.absolute bottom="-5%" left="80%" style="font-size: 0.6em;" .fragment fragment-index=1}
New representation
:::


## {data-menu-title="Attention is (still) all you need" background-image="figures/attention-arxiv.png" background-size="contain"}

:::{.absolute top="0%" left="40%" width="450px"}
![](figures/attention.png){style="background-color: #fff; box-shadow: 10px 10px 10px rgba(0, 0, 0, 0.15); border: 1px solid #9E9E9E;"}
:::



## Training LLMs 

Modern [_(decoder-type)_]{.color-text-muted} language models are [next word predictors]{.pink}.

::::{.columns}
:::{.column width="60%"}

:::{.fragment fragment-index=1}
So we train them to do exactly this:

- Mask the end of sentences
- Use the next token as the prediction target 
- Reveal token and move to next one
- Continue until end of text
:::
:::

:::{.column width="40%"}
![](figures/mask-matrix.png){fig-align="right" .fragment fragment-index=1 style="padding-top: 40px;"}
:::
::::


## Generating text from a decoder model

Modern [_(decoder-type)_]{.color-text-muted} language models are [next word predictors]{.pink}.

Output is a vector of softmax scores for all possible tokens.

:::{style="padding-top: 20px;"}
:::

![](figures/textgen.png){fig-aling="center" width="90%"}

[_Deep Learning with Python_, F. Chollet]{.absolute bottom="0%" left="0%" style="font-size: 0.5em;"}



## Generating text from a decoder model

- Process sequence and predict next token
- Append predicted token to the sequence 
- Process extended sequence and again append predicted token
- Repeat 

:::{.absolute top="50%" left="10%"}
The cat sat on the _____
:::
:::{.absolute top="50%" left="52%" width="300px"}
`floor       7.72%`<br>
`bed         6.82%`<br>
`couch       5.70%`<br>
`ground      4.71%`<br>
`edge        4.66%`<br>
:::


## Generating text from a decoder model

- Process sequence and predict next token
- Append predicted token to the sequence 
- Process extended sequence and again append predicted token
- Repeat 

:::{.absolute top="50%" left="10%"}
The cat sat on the [floor]{.color-pink-dark} _____
:::
:::{.absolute top="50%" left="52%" width="300px"}
`,           25.08%`<br>
`and         13.56%`<br>
`.            7.38%`<br>
`of           7.07%`<br>
`with         6.58%`<br>
:::


## Generating text from a decoder model

- Process sequence and predict next token
- Append predicted token to the sequence 
- Process extended sequence and again append predicted token
- Repeat 

:::{.absolute top="50%" left="10%"}
The cat sat on the [floor, ]{.color-pink-dark} _____
:::
:::{.absolute top="50%" left="52%" width="300px"}
`and         9.41%`<br>
`looking     3.23%`<br>
`the         1.78%`<br>
`he          1.63%`<br>
`his         1.48%`<br>
:::

## Generating text from a decoder model

- Process sequence and predict next token
- Append predicted token to the sequence 
- Process extended sequence and again append predicted token
- Repeat 

:::{.absolute top="50%" left="10%"}
The cat sat on the [floor, and]{.color-pink-dark} _____
:::
:::{.absolute top="50%" left="52%" width="300px"}
`the         8.40%`<br>
`he          5.72%`<br>
`I           4.55%`<br>
`she         3.65%`<br>
`his         3.56%`<br>
:::



## Generating text from a decoder model

[But:]{.red}

Always selecting the single most probable word, often results in an unnatural, potentially meaningless sentence.

![](figures/beamsearch.png)

:::{.fragment}
Need a _sampling strategy_
:::


## Generating text from a decoder model

Simple is often best:

Treat the output scores as probabilities, and [sample randomly]{.pink}.


:::{style="padding-top: 40px;"}
:::

::::{.columns}
:::{.column width="40%"}
:::{.fragment fragment-index=1}
[Tweak:]{.blue}

Add a parameter to tune the probabilities (called _temperature_)
:::
:::

:::{.column width="60%"}
:::{.fragment fragment-index=1}
<iframe width="100%" height="300px" src="figures/softmax_temperature.html"></iframe>
:::
:::
::::


## Today's question {background-color="#E1F5FE"}

:::{style="padding-top: 40px;"}
:::

Using what you have learned from the course, can you make a **_better sampling strategy_**?


